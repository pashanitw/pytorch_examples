{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-18T04:09:32.361160459Z",
     "start_time": "2023-09-18T04:09:32.275174467Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from thop import profile\n",
    "from torchprofile import profile_macs\n",
    "import torch.nn.functional as F\n",
    "from fvcore.nn import flop_count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8a72c4ce73c2c7e",
   "metadata": {},
   "source": [
    "## Demonstrating the Calculation of MACs and FLOPs in Neural Networks with Pen and Paper\n",
    "\n",
    "### Why is Understanding MACs and FLOPs in Neural Networks Important?\n",
    "In this session, we are going to delve deep into the concepts of MACs (Multiply-Accumulate Operations) and FLOPs (Floating Point Operations) within the context of neural networks. By learning how to calculate these manually using pen and paper, you'll acquire a foundational understanding of the computational complexity and efficiency of various network structures.\n",
    "\n",
    "Understanding MACs and FLOPs is not just an academic exercise; it is a critical component in optimizing neural networks for performance and efficiency. It helps in designing models that are both computationally efficient and effective, ultimately saving time and resources during the training and inference phases.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31884e456dfd0f19",
   "metadata": {},
   "source": [
    "# Resource Efficiency:\n",
    "\n",
    "### Computational Efficiency:\n",
    "Understanding FLOPs helps in estimating the computational cost of a neural network. By optimizing the number of FLOPs, one can potentially reduce the time taken to train or run a neural network.\n",
    "\n",
    "## Memory Efficiency:\n",
    "MAC operations often dictate the memory usage of the network since they are directly related to the number of parameters and activations in the network. Reducing MACs can help in making the network memory efficient.\n",
    "\n",
    "# Energy Consumption:\n",
    "\n",
    "## Power Efficiency:\n",
    "Both FLOPs and MAC operations contribute to the power consumption of the hardware on which the neural network is running. By optimizing these metrics, one can potentially reduce the energy requirements of running the network, which is particularly important in mobile and embedded devices.\n",
    "\n",
    "# Model Optimization:\n",
    "\n",
    "## Pruning and Quantization:\n",
    "Understanding FLOPs and MACs can assist in optimizing a neural network through techniques like pruning (removing unnecessary connections) and quantization (reducing the precision of weights and activations), which aim to reduce computational and memory costs.\n",
    "\n",
    "# Performance Benchmarking:\n",
    "\n",
    "## Comparison between Models:\n",
    "FLOPs and MACs provide a means to compare different models in terms of their computational complexity, which can be a criterion for selecting models for specific applications.\n",
    "\n",
    "## Hardware Benchmarking:\n",
    "These metrics can also be used to benchmark the performance of different hardware platforms in running neural networks.\n",
    "\n",
    "# Deployment on Edge Devices:\n",
    "\n",
    "## Real-time Applications:\n",
    "For real-time applications, especially on edge devices with limited computational resources, understanding and optimizing these metrics is critical in ensuring that the network can run within the time constraints of the application.\n",
    "\n",
    "## Battery Life:\n",
    "In battery-powered devices, reducing the computational cost (and hence energy consumption) of neural networks can help in extending the battery life.\n",
    "\n",
    "# Research and Development:\n",
    "\n",
    "## Designing New Algorithms:\n",
    "Researchers can use these metrics as guidelines when developing new algorithms or neural network architectures, aiming to improve computational efficiency without sacrificing accuracy.\n",
    "\n",
    "## Custom Hardware Development:\n",
    "Understanding these metrics can also guide the development of custom hardware (like ASICs or FPGAs) that is optimized for running neural networks with a particular computational profile.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35931390e920da37",
   "metadata": {},
   "source": [
    "## Step 1: Understand the Definitions\n",
    "\n",
    "### FLOP\n",
    "A FLOP (Floating Point OPeration) is considered to be either an addition, subtraction, multiplication, or division operation.\n",
    "\n",
    "### MAC\n",
    "A MAC (Multiply-ACCumulate) operation is essentially a multiplication followed by an addition, i.e., MAC = a * b + c. It counts as two FLOPs (one for multiplication and one for addition).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step 2: Analyze Each Layer\n",
    "\n",
    "\n",
    "### 1. Fully Connected Layer (Dense Layer)\n",
    "Now, we will create a simple neural network with 3 layers and begin counting the operations involved. Here is the formula for calculating the operations in the first linear layer, which is a fully connected (or dense) layer:\n",
    "\n",
    "For a fully connected layer with `I` inputs and `O` outputs, the number of operations are as follows:\n",
    "\n",
    "- **MACs**: `I × O`\n",
    "- **FLOPs**: `2 × (I × O)` (since each MAC counts as two FLOPs)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b6d84339199695b9"
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "4d9816d072fbb80d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-18T07:26:54.468879049Z",
     "start_time": "2023-09-18T07:26:54.425713500Z"
    }
   },
   "outputs": [],
   "source": [
    "class SimpleLinearModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleLinearModel,self).__init__()\n",
    "        self.fc1 = nn.Linear(in_features=10, out_features=20, bias=False)\n",
    "        self.fc2 = nn.Linear(in_features=20, out_features=15, bias=False)\n",
    "        self.fc3 = nn.Linear(in_features=15, out_features=1, bias=False)\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        F.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "ba411f6ed1d75a7e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-18T07:26:54.995254354Z",
     "start_time": "2023-09-18T07:26:54.993235059Z"
    }
   },
   "outputs": [],
   "source": [
    "linear_model = SimpleLinearModel().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "562797087bd58f79",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-18T07:26:55.751357245Z",
     "start_time": "2023-09-18T07:26:55.747898696Z"
    }
   },
   "outputs": [],
   "source": [
    "sample_data = torch.randn(1, 10).cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "**step 1: Identifying Layer Parameters**\n",
    "For the given model, we have three linear layers defined as:\n",
    "\n",
    "- `fc1`: 10 input features, 20 output features\n",
    "- `fc2`: 20 input features, 15 output features\n",
    "- `fc3`: 15 input features, 1 output feature\n",
    "\n",
    "**Step 2: Calculating FLOPs and MACs**\n",
    "Now, calculate MACs and FLOPs for each layer:\n",
    "\n",
    "- Layer `fc1`:\n",
    "  - MACs = 10 × 20 = 200\n",
    "  - FLOPs = 2 × MACs = 2 × 200 = 400\n",
    "\n",
    "- Layer `fc2`:\n",
    "  - MACs = 20 × 15 = 300\n",
    "  - FLOPs = 2 × MACs = 2 × 300 = 600\n",
    "\n",
    "- Layer `fc3`:\n",
    "  - MACs = 15 × 1 = 15\n",
    "  - FLOPs = 2 × MACs = 2 × 15 = 30\n",
    "\n",
    "**Step 3: Summing Up the Results**\n",
    "Finally, to find the total number of MACs and FLOPs for a single input passing through the entire network, we sum the results from all layers:\n",
    "\n",
    "- Total MACs = MACs(`fc1`) + MACs(`fc2`) + MACs(`fc3`) = 200 + 300 + 15 = 515\n",
    "- Total FLOPs = FLOPs(`fc1`) + FLOPs(`fc2`) + FLOPs(`fc3`) = 400 + 600 + 30 = 1030"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8dc269c9ff7d9593"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Verifying FLOPs and MACs with `torchprofile` Library\n",
    "\n",
    "You can use the `torchprofile` library to verify the FLOPs and MACs calculations for the given neural network model. Here's how to do it:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "185b3f35be368c53"
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "94474c5f7a30cea5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-18T08:03:45.427140425Z",
     "start_time": "2023-09-18T08:03:45.386716267Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "515\n"
     ]
    }
   ],
   "source": [
    "macs = profile_macs(linear_model, sample_data)\n",
    "print(macs)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Calculating MACs and FLOPs for a Simple Convolutional Model\n",
    "\n",
    "Now, let's determine the MACs (Multiply-Accumulates) and FLOPs (Floating-Point Operations) for a straightforward convolutional model. This calculation can be a bit more involved than our previous example with dense layers, mainly due to factors like stride, padding, and kernel size. However, I'll break it down to make it easier for our learning purpose.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6f6b93f97dde230"
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "e2a444dec95cac22",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-18T08:03:46.862255093Z",
     "start_time": "2023-09-18T08:03:46.860227029Z"
    }
   },
   "outputs": [],
   "source": [
    "class SimpleConv(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleConv, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=16, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
    "        self.fc =  nn.Linear(in_features=32*28*28, out_features=10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "a9ecb218b7e99ba1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-18T07:32:40.467321566Z",
     "start_time": "2023-09-18T07:32:40.425454616Z"
    }
   },
   "outputs": [],
   "source": [
    "x = torch.rand(1, 1, 28, 28).cuda()\n",
    "conv_model = SimpleConv().cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Important Consideration for Calculating Convolutional Operations\n",
    "\n",
    "* When calculating operations for convolutional kernels, it's crucial to remember that the number of channels in the kernel should match the number of channels in the input. For instance, if our input is an RGB image with three color channels, the kernel's dimensions will be `3x3x3` to account for the input's three channels.\n",
    "* For the purpose of our demonstration, we'll maintain a consistent image size throughout the convolutional layers. To achieve this, we'll set both the padding and stride values to 1.\n",
    "\n",
    "**step 1: Identifying Layer Parameters**\n",
    "\n",
    "For the given model, we have two conv layers and one linear layer defined as:\n",
    "\n",
    "- `conv1`: 1 input channels, 16 output channels, kernel size is 3 \n",
    "- `conv2`: 16 input channels, 32 output channels\n",
    "- `fc`: `32*28*28` input features, 1 output feature. because our image is not changed in the convolutional layers\n",
    "\n",
    "**Step 2: Calculating FLOPs and MACs**\n",
    "Now, calculate MACs and FLOPs for each layer:\n",
    "\n",
    "*** formula is `output_image_size * kernel shape * output_channels`  \n",
    "- Layer `conv1`:\n",
    "  - MACs = 28 * 28 * 3 * 3 * 1 * 16 = 1,12,896 \n",
    "  - FLOPs = 2 × MACs = 2 × 200 = 2,25,792\n",
    "\n",
    "- Layer `conv2`:\n",
    "  - MACs = 28 × 28 * 3 * 3 * 16 * 32  = 3,612,672\n",
    "  - FLOPs = 2 × MACs = 2 × 300 = 600 = 7,225,344\n",
    "\n",
    "- Layer `fc`:\n",
    "  - MACs = 32 * 28 * 28 * 10 = 250,880\n",
    "  - FLOPs = 2 × MACs = 2 × 15 = 501,760\n",
    "\n",
    "**Step 3: Summing Up the Results**\n",
    "Finally, to find the total number of MACs and FLOPs for a single input passing through the entire network, we sum the results from all layers:\n",
    "\n",
    "- Total MACs = MACs(`conv1`) + MACs(`conv2`) + MACs(`fc`) = 1,12,896 + 3,612,672 + 250,880 = 39,76,448\n",
    "- Total FLOPs = FLOPs(`fc1`) + FLOPs(`fc2`) + FLOPs(`fc3`) = 2,25,792 + 7,225,344 + 501,760 = 7,952,896"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e4cf7acf7262481c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Verifying Operations with `torchprofile` Library"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "442cb74a7f163484"
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "18cdce52d0fe56d5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-18T08:47:08.059196083Z",
     "start_time": "2023-09-18T08:47:08.016074930Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "3976448"
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "macs = profile_macs(conv_model,(x,))\n",
    "macs"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Calculating FLOPs for a Self-Attention Block\n",
    "\n",
    "Having covered MACs for linear and convolutional layers, our next step is to determine the FLOPs (Floating-Point Operations) for a Self-Attention block, a crucial component in large language models. This calculation is essential for understanding the computational complexity of such models. Let's delve into it.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "67112e2d447d0baf"
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "ffbcd9cbeb8ca55b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-18T08:47:08.734285845Z",
     "start_time": "2023-09-18T08:47:08.732663531Z"
    }
   },
   "outputs": [],
   "source": [
    "class SimpleAttentionBlock(nn.Module):\n",
    "    def __init__(self, embed_size, heads):\n",
    "        super(SimpleAttentionBlock, self).__init__()\n",
    "        self.embed_size = embed_size\n",
    "        self.heads = heads\n",
    "        self.head_dim = embed_size // heads\n",
    "\n",
    "        assert (\n",
    "            self.head_dim * heads == embed_size\n",
    "        ), \"Embedding size needs to be divisible by heads\"\n",
    "\n",
    "        self.values = nn.Linear(self.embed_size, self.embed_size, bias=False)\n",
    "        self.keys = nn.Linear(self.embed_size, self.embed_size, bias=False)\n",
    "        self.queries = nn.Linear(self.embed_size, self.embed_size, bias=False)\n",
    "        self.fc_out = nn.Linear(heads * self.head_dim, embed_size)\n",
    "\n",
    "    def forward(self, values, keys, queries, mask):\n",
    "        N = queries.shape[0]\n",
    "        value_len, key_len, query_len = values.shape[1], keys.shape[1], queries.shape[1]\n",
    "        print(values.shape)\n",
    "        values = self.values(values).reshape(N,  self.heads, value_len, self.head_dim)\n",
    "        keys = self.keys(keys).reshape(N, self.heads, key_len, self.head_dim)\n",
    "        queries = self.queries(queries).reshape(N,  self.heads, query_len, self.head_dim)\n",
    "\n",
    "\n",
    "        energy = torch.matmul(queries, keys.transpose(-2, -1))        \n",
    "\n",
    "        if mask is not None:\n",
    "            energy = energy.masked_fill(mask == 0, float(\"-1e20\"))\n",
    "\n",
    "        attention = torch.nn.functional.softmax(energy, dim=3)\n",
    "        out = torch.matmul(attention, values).reshape(\n",
    "            N, query_len, self.heads * self.head_dim\n",
    "        )\n",
    "\n",
    "        return self.fc_out(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "**step 1: Identifying Layer Parameters**\n",
    "### Linear Transformations\n",
    "* let's define hyper_params\n",
    "* batch_size = 1\n",
    "* seq_len = 10\n",
    "* embed_size = 256\n",
    "In the attention block, we have three linear transformations (for queries, keys, and values), and one at the end (fc_out).\n",
    "* Input Size: [batch_size, seq_len, embed_size]\n",
    "* Linear transformation matrix: [embed_size, embed_size]\n",
    "* MACs: batch_size×seq_len×embed_size×embed_size\n",
    "- Query, Key, Value `linear transformation`:\n",
    "  - MACs for Query Transformation = 1 * 10 * 256 * 256 = 6,55,360\n",
    "  - MACs for Key Transformation = 1 * 10 * 256 * 256 = 6,55,360\n",
    "  _ MACS for Value Transformation = 1 * 10 * 256 * 256 = 6,55,360\n",
    "\n",
    "* Energy Calculation\n",
    "    Calculation: queries (reshaped) dot keys (reshaped) - a dot product operation.\n",
    "    * Macs: batch_size×seq_len×seq_len×heads×head_dim\n",
    "    - query and key dot product\n",
    "      - MACS = 1 * 10 * 10 * 32 [32 because 256/8 divide by heads] = 25,600\n",
    "\n",
    "* Output from Attention Weights and Values\n",
    "    Calculation: attention weights dot values (reshaped) - another dot product operation.\n",
    "    * Macs : batch_size×seq_len×seq_len×heads×head_dim\n",
    "    - attention and value dot product\n",
    "      - Macs = 1 * 10 * 10 * 32 = 25,600\n",
    "\n",
    "* Fully Connected Output (fc_out)\n",
    "    * Macs: batch_size×seq_len×heads×head_dim×embed_size\n",
    "    - fc_out\n",
    "      - Macs = 1 * 10 * 8 * 32 * 256  = 6,55,360\n",
    "\n",
    "**Step 3: Summing Up the Results**\n",
    "\n",
    "- Total MACs = MACs(`conv1`) + MACs(`conv2`) + MACs(`fc`) = 6,55,360 + 6,55,360 + 6,55,360 + 25,600 + 25,600 + 6,55,360 = 26,72,640\n",
    "- Total FLOPs =  2 * Total MACs = 53,45,280\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6191ce885b4e2ed7"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Verifying Operations with `torchprofile` Library"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c696db61ed0610bb"
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "outputs": [],
   "source": [
    "# Create an instance of the model\n",
    "model = SimpleAttentionBlock(embed_size=256, heads=8).cuda()\n",
    "\n",
    "# Generate some sample data (batch of 5 sequences, each of length 10, embedding size 256)\n",
    "values = torch.randn(1, 10, 256).cuda()\n",
    "keys = torch.randn(1, 10, 256).cuda()\n",
    "queries = torch.randn(1, 10, 256).cuda()\n",
    "\n",
    "# No mask for simplicity\n",
    "mask = None"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-18T08:52:50.014997924Z",
     "start_time": "2023-09-18T08:52:49.974197186Z"
    }
   },
   "id": "605e3647c53effff"
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "4285320157ecd8bc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-18T08:52:50.456276796Z",
     "start_time": "2023-09-18T08:52:50.450580004Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 10, 256])\n",
      "2672640\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kuro/miniconda3/envs/test/lib/python3.10/site-packages/torchprofile/profile.py:22: UserWarning: No handlers found: \"aten::reshape\". Skipped.\n",
      "  warnings.warn('No handlers found: \"{}\". Skipped.'.format(\n"
     ]
    }
   ],
   "source": [
    "# Forward pass with the sample data\n",
    "macs = profile_macs(model, (values, keys, queries, mask))\n",
    "print(macs)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Summary: Scaling MACs and FLOPs for Different Batch Sizes\n",
    "\n",
    "Throughout our calculations, we've primarily considered a batch size of 1. However, it's important to note that scaling MACs and FLOPs for larger batch sizes is straightforward. \n",
    "\n",
    "To compute MACs or FLOPs for a batch size greater than one, you can simply multiply the total MACs or FLOPs obtained for batch size 1 by the desired batch size value. This scaling allows you to estimate computational requirements for various batch sizes in your neural network models.\n",
    "\n",
    "Keep in mind that the results will directly scale linearly with the batch size. For instance, if you have a batch size of 32, you can obtain the MACs or FLOPs by multiplying the values for batch size 1 by 32.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6aed7ee06f163e32"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "b7a8c1c68dddb83c"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
