{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Part 1: Building Vision Transformers from Scratch: A PyTorch Deep Dive Plus a Teaser on LORA for Part 2\n",
    "\n",
    "If you've delved into the realm of deep learning, you're likely aware of the  impact that transformer architectures have had on the field of artificial intelligence. These architectures stand at the core of numerous groundbreaking advancements in AI. In this Article, we will embark on an in-depth exploration, guiding you through the process of building Vision Transformers from the ground up.\n",
    "\n",
    "This article is the first in a four-part series. The next one will show how to build 'LoRa' from scratch, for the Vision Transformer we are building here."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6b324adab237dd18"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-11-01T13:32:05.220439538Z",
     "start_time": "2023-11-01T13:32:04.093402934Z"
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import torch \n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import CIFAR10\n",
    "from torch import nn\n",
    "from dataclasses import dataclass\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "# Define a configuration for the model using a data class\n",
    "@dataclass\n",
    "class ModelArgs:\n",
    "    dim: int = 256          # Dimension of the model embeddings\n",
    "    hidden_dim: int = 512   # Dimension of the hidden layers\n",
    "    n_heads: int = 8        # Number of attention heads\n",
    "    n_layers: int = 6       # Number of layers in the transformer\n",
    "    patch_size: int = 4     # Size of the patches (typically square)\n",
    "    n_channels: int = 3     # Number of input channels (e.g., 3 for RGB images)\n",
    "    n_patches: int = 64     # Number of patches in the input\n",
    "    n_classes: int = 10     # Number of target classes\n",
    "    dropout: float = 0.2    # Dropout rate for regularization\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-01T13:32:17.687483095Z",
     "start_time": "2023-11-01T13:32:17.682905721Z"
    }
   },
   "id": "c973a30501c1fd03"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## **MultiHead Attention Overview**\n",
    "\n",
    "The `MultiHeadAttention` module in the provided code is an implementation of the multi-head self-attention mechanism, which stands as a fundamental component in transformer architectures. This self-attention mechanism empowers the model to weigh input elements differently, offering the capability to focus more intently on certain parts of the input when generating the output.\n",
    "\n",
    "First, let's code the multi-head attention block. Afterward, I'll break down the key components in detail"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5c5a6a97fdf291c"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, args:ModelArgs):\n",
    "        super().__init__()\n",
    "        self.n_heads = args.n_heads\n",
    "        self.dim = args.dim\n",
    "        self.head_dim = args.dim // args.n_heads\n",
    "        \n",
    "        # Linear projections for Q, K, and V\n",
    "        self.wq = nn.Linear(self.dim, self.n_heads*self.head_dim, bias=False)\n",
    "        self.wk = nn.Linear(self.dim, self.n_heads*self.head_dim, bias=False)\n",
    "        self.wv = nn.Linear(self.dim, self.n_heads*self.head_dim, bias=False)\n",
    "        self.wo = nn.Linear(self.n_heads*self.head_dim, self.dim, bias=False)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        b, seq_len, dim = x.shape  # b: batch size, seq_len: sequence length\n",
    "        \n",
    "        assert dim == self.dim, \"dim is not matching\"\n",
    "        \n",
    "        q = self.wq(x)  # [b, seq_len, n_heads*head_dim]\n",
    "        k = self.wk(x)  # [b, seq_len, n_heads*head_dim]\n",
    "        v = self.wv(x)  # [b, seq_len, n_heads*head_dim]\n",
    "        \n",
    "        # Reshape the tensors for multi-head operations\n",
    "        q = q.contiguous().view(b, seq_len, self.n_heads, self.head_dim)  # [b, seq_len, n_heads, head_dim]\n",
    "        k = k.contiguous().view(b, seq_len, self.n_heads, self.head_dim)  # [b, seq_len, n_heads, head_dim]\n",
    "        v = v.contiguous().view(b, seq_len, self.n_heads, self.head_dim)  # [b, seq_len, n_heads, head_dim]\n",
    "        \n",
    "        # Transpose to bring the head dimension to the front\n",
    "        q = q.transpose(1, 2)  # [b, n_heads, seq_len, head_dim]\n",
    "        k = k.transpose(1, 2)  # [b, n_heads, seq_len, head_dim]\n",
    "        v = v.transpose(1, 2)  # [b, n_heads, seq_len, head_dim]\n",
    "        \n",
    "        # Compute attention scores and apply softmax\n",
    "        attn = torch.matmul(q, k.transpose(2, 3)) / math.sqrt(self.head_dim)  # [b, n_heads, seq_len, seq_len]\n",
    "        attn_scores = F.softmax(attn, dim=-1)  # [b, n_heads, seq_len, seq_len]\n",
    "        \n",
    "        # Compute the attended features\n",
    "        out = torch.matmul(attn_scores, v)  # [b, n_heads, seq_len, head_dim]\n",
    "        out = out.contiguous().view(b, seq_len, -1)  # [b, seq_len, n_heads*head_dim]\n",
    "        \n",
    "        return self.wo(out)  # [b, seq_len, dim]\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-01T13:32:21.126785745Z",
     "start_time": "2023-11-01T13:32:21.125328320Z"
    }
   },
   "id": "19ec9440213a820a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "The MultiHeadAttention module performing the following operations:\n",
    "1. Linear transformations of the input tensor into **\"query\" (Q)**, **\"key\" (K)**, and **\"value\" (V)** representations.\n",
    "    ```python\n",
    "       q = self.wq(x)\n",
    "       k = self.wk(x)\n",
    "       v = self.wv(x)\n",
    "    ```\n",
    "2. Dividing these tensors into multiple \"heads\".\n",
    "    ```python\n",
    "        q = q.contiguous().view(b, seq_len, self.n_heads, self.head_dim)\n",
    "        k = k.contiguous().view(b, seq_len, self.n_heads, self.head_dim)\n",
    "        v = v.contiguous().view(b, seq_len, self.n_heads, self.head_dim)\n",
    "    ```\n",
    "3. Computing attention scores via the dot product of Q and K.\n",
    "    ```python\n",
    "        attn = torch.matmul(q, k.transpose(2, 3)) / math.sqrt(self.head_dim)\n",
    "    ```\n",
    "4. Applying softmax to these scores to procure attention weights.\n",
    "    ```python\n",
    "        attn_scores = F.softmax(attn, dim=-1)\n",
    "    ```\n",
    "5. Multiplying the attention weights with the V tensor, yielding the attended features.\n",
    "    ```python\n",
    "        out = torch.matmul(attn_scores, v)\n",
    "    ```\n",
    "6. Aggregating results across all heads and projecting to provide the concluding output.\n",
    "    ```python\n",
    "        out = out.contiguous().view(b, seq_len, -1)\n",
    "        return self.wo(out)\n",
    "    ```"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "95ad8d03e60a41d7"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## **AttentionBlock Overview**\n",
    "\n",
    "The `AttentionBlock` module encapsulates a typical block found within transformer architectures. It primarily consists of two significant components: a multi-head self-attention mechanism and a feed-forward neural network (FFN). Additionally, layer normalization and skip connections (residual connections) are employed to facilitate better learning and gradient flow.\n",
    "\n",
    "let's code the multi-head attention block.\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2b877ce8bf0e63fa"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "class AttentionBlock(nn.Module):\n",
    "    def __init__(self, args: ModelArgs):\n",
    "        super().__init__()\n",
    "        self.layer_norm_1 = nn.LayerNorm(args.dim)\n",
    "        self.attn = MultiHeadAttention(args)\n",
    "        \n",
    "        self.layer_norm_2 = nn.LayerNorm(args.dim)\n",
    "        \n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(args.dim, args.hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(args.dropout),\n",
    "            nn.Linear(args.hidden_dim, args.dim),\n",
    "            nn.Dropout(args.dropout)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.layer_norm_1(x))\n",
    "        x = x + self.ffn(self.layer_norm_2(x))\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-01T13:32:24.966544849Z",
     "start_time": "2023-11-01T13:32:24.962034957Z"
    }
   },
   "id": "4c3f7596b4ec9009"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's delve deeper into its structure:\n",
    "\n",
    "1. **Layer Normalization (Pre-Attention)**\n",
    "   Before feeding the input `x` into the multi-head attention mechanism, it's normalized using `LayerNorm`.\n",
    "   ```python\n",
    "   self.layer_norm_1 = nn.LayerNorm(args.dim)\n",
    "   x = self.layer_norm_1(x)\n",
    "   ```\n",
    "\n",
    "2. **Multi-Head Self-Attention**\n",
    "   This component allows the model to focus on different parts of the input sequence when generating its output.\n",
    "   ```python\n",
    "   self.attn = MultiHeadAttention(args)\n",
    "   x = x + self.attn(x)\n",
    "   ```\n",
    "\n",
    "3. **Layer Normalization (Pre-Feed-Forward Network)**\n",
    "   Just like before the multi-head attention mechanism, the output is normalized again using `LayerNorm` before feeding it into the FFN.\n",
    "   ```python\n",
    "   self.layer_norm_2 = nn.LayerNorm(args.dim)\n",
    "   x = self.layer_norm_2(x)\n",
    "   ```\n",
    "\n",
    "4. **Feed-Forward Neural Network (FFN)**\n",
    "   The FFN consists of two linear layers separated by a GELU activation function. There's also dropout applied for regularization.\n",
    "   ```python\n",
    "   self.ffn = nn.Sequential(\n",
    "       nn.Linear(args.dim, args.hidden_dim),\n",
    "       nn.GELU(),\n",
    "       nn.Dropout(args.dropout),\n",
    "       nn.Linear(args.hidden_dim, args.dim),\n",
    "       nn.Dropout(args.dropout)\n",
    "   )\n",
    "   x = x + self.ffn(x)\n",
    "   ```\n",
    "\n",
    "5. **Residual Connections**\n",
    "   Residual or skip connections are vital for deep architectures like transformers. They help in preventing the vanishing gradient problem and aid in model convergence. In the code, these are represented by the addition operations where the input is added back to the output of both the attention mechanism and the FFN.\n",
    "   ```python\n",
    "   x = x + self.attn(...)\n",
    "   x = x + self.ffn(...)\n",
    "   ```\n",
    "\n",
    "By sequentially organizing the operations, this block ensures efficient and effective feature transformation, which is essential for the transformer's performance.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5b997e928ad738b3"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Before creating our full vistion transformer model, we need to create a utility function that transforms images into non-overlapping patches."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ed38fce5d2d6e48a"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "def img_to_patch(x, patch_size, flatten_channels=True):\n",
    "    # x: Input image tensor \n",
    "    # B: Batch size, C: Channels, H: Height, W: Width\n",
    "    B, C, H, W = x.shape  # (B, C, H, W)\n",
    "    \n",
    "    # Reshape the image tensor to get non-overlapping patches\n",
    "    x = x.reshape(B, C, H//patch_size, patch_size, W//patch_size, patch_size)  # (B, C, H/patch_size, patch_size, W/patch_size, patch_size)\n",
    "    \n",
    "    # Permute to group the patches and channels\n",
    "    x = x.permute(0, 2, 4, 1, 3, 5)  # (B, H/patch_size, W/patch_size, C, patch_size, patch_size)\n",
    "    \n",
    "    # Flatten the height and width dimensions for patches\n",
    "    x = x.flatten(1,2)  # (B, (H/patch_size * W/patch_size), C, patch_size, patch_size)\n",
    "    \n",
    "    # Option to flatten the channel and spatial dimensions\n",
    "    if flatten_channels:\n",
    "        x = x.flatten(2,4)  # (B, (H/patch_size * W/patch_size), (C * patch_size * patch_size))\n",
    "    \n",
    "    return x"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-01T13:32:27.019168995Z",
     "start_time": "2023-11-01T13:32:27.016198213Z"
    }
   },
   "id": "1e0a47106ccf6b12"
  },
  {
   "cell_type": "markdown",
   "source": [
    "The img_to_patch function takes an image tensor and converts it into non-overlapping patches of a specified size. This operation is typically used in vision transformers to represent an image as a sequence of flattened patches. The function provides an option to flatten the channels or keep them separate."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8125e6b13ada1e6"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## **VisionTransformer Overview**\n",
    "\n",
    "The `VisionTransformer` effectively integrates the previously discussed components to construct the final model. It operates as an encoder-only architecture similar to BERT, where all tokens attend to all other tokens. Moreover, we introduce an additional class token (`cls_token`) to every sequence in the batch, and this will be utilized later for classification, much like how BERT does with its special [CLS] token.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d60443199f9ba889"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "class VisionTransformer(nn.Module):\n",
    "    def __init__(self, args: ModelArgs):\n",
    "        super().__init__()\n",
    "\n",
    "        # Define the patch size\n",
    "        self.patch_size = args.patch_size\n",
    "        \n",
    "        # Embedding layer to transform flattened patches to desired dimension\n",
    "        self.input_layer = nn.Linear(args.n_channels * (args.patch_size ** 2), args.dim)\n",
    "\n",
    "        # Create the attention blocks for the transformer\n",
    "        attn_blocks = []\n",
    "        for _ in range(args.n_layers):\n",
    "            attn_blocks.append(AttentionBlock(args))\n",
    "        \n",
    "        # Create the transformer by stacking the attention blocks\n",
    "        self.transformer = nn.Sequential(*attn_blocks)\n",
    "        \n",
    "        # Define the classifier\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.LayerNorm(args.dim),\n",
    "            nn.Linear(args.dim, args.n_classes)\n",
    "        )\n",
    "        \n",
    "        # Dropout layer for regularization\n",
    "        self.dropout = nn.Dropout(args.dropout)\n",
    "        \n",
    "        # Define the class token (similar to BERT's [CLS] token)\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, args.dim))\n",
    "        \n",
    "        # Positional embeddings to give positional information to transformer\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1, 1+args.n_patches, args.dim))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Convert image to patches and flatten\n",
    "        x = img_to_patch(x, self.patch_size)\n",
    "        b, seq_len, _ = x.shape\n",
    "        \n",
    "        # Transform patches using the embedding layer\n",
    "        x = self.input_layer(x)\n",
    "        \n",
    "        # Add the class token to the beginning of each sequence\n",
    "        cls_token = self.cls_token.repeat(b, 1, 1)\n",
    "        x = torch.cat([cls_token, x], dim=1)\n",
    "        \n",
    "        # Add positional embeddings to the sequence\n",
    "        x = x + self.pos_embedding[:,:seq_len+1]\n",
    "        \n",
    "        # Apply dropout\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # Process sequence through the transformer\n",
    "        x = self.transformer(x)\n",
    "\n",
    "        # Retrieve the class token's representation (for classification)\n",
    "        x = x.transpose(0, 1)\n",
    "        cls = x[0]\n",
    "\n",
    "        # Classify using the representation of the class token\n",
    "        out = self.mlp(cls)\n",
    "        return out"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-01T13:32:28.914633924Z",
     "start_time": "2023-11-01T13:32:28.910704873Z"
    }
   },
   "id": "b51bde2d3d36e9c9"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's dive into its key aspects:\n",
    " \n",
    "1. **Patch Embedding**\n",
    "    Instead of operating on raw pixels, the image is divided into fixed-size patches. Each patch is then linearly transformed (flattened and passed through a linear layer) to a specified dimension (`args.dim`).\n",
    "   ```python\n",
    "   self.patch_size = args.patch_size\n",
    "   self.input_layer = nn.Linear(args.n_channels * (args.patch_size ** 2), args.dim)\n",
    "   x = img_to_patch(x, self.patch_size)  # Assuming `img_to_patch` is a helper function.\n",
    "   x = self.input_layer(x)\n",
    "   ```\n",
    "\n",
    "2. **Transformer Blocks**\n",
    "   A sequence of attention blocks to process the embedded patches. The number of blocks is defined by `args.n_layers`.\n",
    "   ```python\n",
    "   attn_blocks = []\n",
    "   for _ in range(args.n_layers):\n",
    "       attn_blocks.append(AttentionBlock(args))\n",
    "   self.transformer = nn.Sequential(*attn_blocks)\n",
    "   x = self.transformer(x)\n",
    "   ```\n",
    "\n",
    "3. **CLS Token and Position Embeddings**\n",
    "   A class token is added to the sequence of embedded patches. This token is later used to obtain the final classification output. Positional embeddings are added to provide the transformer with information about the relative positions of patches.\n",
    "   ```python\n",
    "   self.cls_token = nn.Parameter(torch.randn(1, 1, args.dim))\n",
    "   self.pos_embedding = nn.Parameter(torch.randn(1, 1+args.n_patches, args.dim))\n",
    "   cls_token = self.cls_token.repeat(b, 1, 1)\n",
    "   x = torch.cat([cls_token, x], dim=1)\n",
    "   x = x + self.pos_embedding[:,:seq_len+1]\n",
    "   ```\n",
    "\n",
    "4. **Dropout**\n",
    "   Dropout is applied for regularization purposes.\n",
    "   ```python\n",
    "   self.dropout = nn.Dropout(args.dropout)\n",
    "   x = self.dropout(x)\n",
    "   ```\n",
    "\n",
    "5. **Classifier**\n",
    "   The classification head. It uses the class token's[CLS] representation after it's been processed by all transformer blocks.\n",
    "   ```python\n",
    "   self.mlp = nn.Sequential(\n",
    "       nn.LayerNorm(args.dim),\n",
    "       nn.Linear(args.dim, args.n_classes)\n",
    "   )\n",
    "   x = x.transpose(0, 1)\n",
    "   cls = x[0]\n",
    "   out = self.mlp(cls)\n",
    "   ```\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "824829621ac13a2f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "The Below code snippet provides a setup for preprocessing and loading the CIFAR10 dataset"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b4bef4d8a98e1828"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# Path to the directory where CIFAR10 data will be stored/downloaded\n",
    "DATA_DIR = \"../data\"\n",
    "\n",
    "# Define the transformation for testing dataset:\n",
    "# 1. Convert images to tensors.\n",
    "# 2. Normalize the tensors using the mean and standard deviation of CIFAR10 dataset.\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.49139968, 0.48215841, 0.44653091], [0.24703223, 0.24348513, 0.26158784])\n",
    "])\n",
    "\n",
    "# Define the transformation for training dataset:\n",
    "# 1. Apply random horizontal flip for data augmentation.\n",
    "# 2. Perform random resizing and cropping of images for data augmentation.\n",
    "# 3. Convert images to tensors.\n",
    "# 4. Normalize the tensors using the mean and standard deviation of CIFAR10 dataset.\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomResizedCrop((32, 32), scale=(0.8, 1.0), ratio=(0.9, 1.1)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.49139968, 0.48215841, 0.44653091], [0.24703223, 0.24348513, 0.26158784])\n",
    "])\n",
    "\n",
    "# Load the CIFAR10 training dataset with the defined training transformation.\n",
    "# The dataset will be downloaded if not present in the DATA_DIR.\n",
    "train_dataset = CIFAR10(root=DATA_DIR, train=True, transform=train_transform, download=True)\n",
    "\n",
    "# Load the CIFAR10 testing dataset with the defined testing transformation.\n",
    "# The dataset will be downloaded if not present in the DATA_DIR.\n",
    "test_set = CIFAR10(root=DATA_DIR, train=False, transform=test_transform, download=True)\n",
    "\n",
    "# Split the training dataset into training and validation sets.\n",
    "# The training set will have 45000 images, and the validation set will have 5000 images.\n",
    "train_set, val_set = torch.utils.data.random_split(train_dataset, [45000, 5000])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-01T13:32:37.758891976Z",
     "start_time": "2023-11-01T13:32:36.632403359Z"
    }
   },
   "id": "816aefa5793746e0"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's setup the data loaders for training, validation and test datasets"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bff8dcc9e919ecae"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "# Define the batch size for training, validation, and testing.\n",
    "batch_size = 64\n",
    "\n",
    "# Define the number of subprocesses to use for data loading.\n",
    "num_workers = 16\n",
    "\n",
    "# Create a DataLoader for the training and validation dataset:\n",
    "# 1. Shuffle the training data for each epoch.\n",
    "# 2. Drop the last batch if its size is not equal to `batch_size` to maintain consistency.\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_set, \n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True,\n",
    "                                           num_workers=num_workers, \n",
    "                                           drop_last=True)\n",
    "\n",
    "# Do not drop any data; process all the validation data.\n",
    "val_loader = torch.utils.data.DataLoader(dataset=val_set, \n",
    "                                         batch_size=batch_size, \n",
    "                                         shuffle=False,\n",
    "                                         num_workers=num_workers, \n",
    "                                         drop_last=False)\n",
    "\n",
    "# Create a DataLoader for the testing dataset:\n",
    "# Do not drop any data; process all the test data.\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_set, \n",
    "                                          batch_size=batch_size, \n",
    "                                          shuffle=False,\n",
    "                                          num_workers=num_workers, \n",
    "                                          drop_last=False)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-01T13:32:41.205505797Z",
     "start_time": "2023-11-01T13:32:41.196627932Z"
    }
   },
   "id": "1388884734ae19ea"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's configure the model, optimization strategy, and training criterion."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4b9eb262f8b06f4d"
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "# Model, Loss and Optimizer\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else 0\n",
    "args = ModelArgs()\n",
    "model = VisionTransformer(args).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=3e-4)\n",
    "lr_scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[80, 130], gamma=0.1)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-01T13:55:38.903587158Z",
     "start_time": "2023-11-01T13:55:38.856277365Z"
    }
   },
   "id": "9c6687f6139d368e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Time to bring our model to life! Let's train it."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a014306f05b984fd"
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/150], Training Loss: 1.6899\n",
      "Epoch [1/150], Validation Loss: 1.4533, Validation Accuracy: 47.44%\n",
      "Epoch [2/150], Training Loss: 1.3940\n",
      "Epoch [2/150], Validation Loss: 1.3406, Validation Accuracy: 52.12%\n",
      "Epoch [3/150], Training Loss: 1.2890\n",
      "Epoch [3/150], Validation Loss: 1.2699, Validation Accuracy: 54.58%\n",
      "Epoch [4/150], Training Loss: 1.2265\n",
      "Epoch [4/150], Validation Loss: 1.2358, Validation Accuracy: 55.82%\n",
      "Epoch [5/150], Training Loss: 1.1703\n",
      "Epoch [5/150], Validation Loss: 1.1882, Validation Accuracy: 57.82%\n",
      "Epoch [6/150], Training Loss: 1.1259\n",
      "Epoch [6/150], Validation Loss: 1.1857, Validation Accuracy: 57.64%\n",
      "Epoch [7/150], Training Loss: 1.0941\n",
      "Epoch [7/150], Validation Loss: 1.1299, Validation Accuracy: 60.42%\n",
      "Epoch [8/150], Training Loss: 1.0640\n",
      "Epoch [8/150], Validation Loss: 1.1399, Validation Accuracy: 60.20%\n",
      "Epoch [9/150], Training Loss: 1.0369\n",
      "Epoch [9/150], Validation Loss: 1.0889, Validation Accuracy: 61.30%\n",
      "Epoch [10/150], Training Loss: 1.0141\n",
      "Epoch [10/150], Validation Loss: 1.0968, Validation Accuracy: 61.90%\n",
      "Epoch [11/150], Training Loss: 0.9868\n",
      "Epoch [11/150], Validation Loss: 1.0888, Validation Accuracy: 61.44%\n",
      "Epoch [12/150], Training Loss: 0.9644\n",
      "Epoch [12/150], Validation Loss: 1.0679, Validation Accuracy: 62.32%\n",
      "Epoch [13/150], Training Loss: 0.9433\n",
      "Epoch [13/150], Validation Loss: 1.0241, Validation Accuracy: 64.46%\n",
      "Epoch [14/150], Training Loss: 0.9187\n",
      "Epoch [14/150], Validation Loss: 1.0402, Validation Accuracy: 63.56%\n",
      "Epoch [15/150], Training Loss: 0.8963\n",
      "Epoch [15/150], Validation Loss: 0.9972, Validation Accuracy: 63.82%\n",
      "Epoch [16/150], Training Loss: 0.8820\n",
      "Epoch [16/150], Validation Loss: 1.0196, Validation Accuracy: 64.02%\n",
      "Epoch [17/150], Training Loss: 0.8663\n",
      "Epoch [17/150], Validation Loss: 0.9855, Validation Accuracy: 65.76%\n",
      "Epoch [18/150], Training Loss: 0.8485\n",
      "Epoch [18/150], Validation Loss: 1.0054, Validation Accuracy: 64.94%\n",
      "Epoch [19/150], Training Loss: 0.8254\n",
      "Epoch [19/150], Validation Loss: 0.9780, Validation Accuracy: 65.96%\n",
      "Epoch [20/150], Training Loss: 0.8090\n",
      "Epoch [20/150], Validation Loss: 0.9675, Validation Accuracy: 66.44%\n",
      "Epoch [21/150], Training Loss: 0.7890\n",
      "Epoch [21/150], Validation Loss: 1.0001, Validation Accuracy: 65.78%\n",
      "Epoch [22/150], Training Loss: 0.7722\n",
      "Epoch [22/150], Validation Loss: 0.9517, Validation Accuracy: 67.04%\n",
      "Epoch [23/150], Training Loss: 0.7527\n",
      "Epoch [23/150], Validation Loss: 0.9570, Validation Accuracy: 67.88%\n",
      "Epoch [24/150], Training Loss: 0.7401\n",
      "Epoch [24/150], Validation Loss: 0.9521, Validation Accuracy: 68.02%\n",
      "Epoch [25/150], Training Loss: 0.7248\n",
      "Epoch [25/150], Validation Loss: 0.9360, Validation Accuracy: 68.12%\n",
      "Epoch [26/150], Training Loss: 0.7117\n",
      "Epoch [26/150], Validation Loss: 0.9336, Validation Accuracy: 68.30%\n",
      "Epoch [27/150], Training Loss: 0.6987\n",
      "Epoch [27/150], Validation Loss: 0.9312, Validation Accuracy: 67.50%\n",
      "Epoch [28/150], Training Loss: 0.6816\n",
      "Epoch [28/150], Validation Loss: 0.9451, Validation Accuracy: 67.90%\n",
      "Epoch [29/150], Training Loss: 0.6623\n",
      "Epoch [29/150], Validation Loss: 0.9152, Validation Accuracy: 68.66%\n",
      "Epoch [30/150], Training Loss: 0.6503\n",
      "Epoch [30/150], Validation Loss: 0.9192, Validation Accuracy: 69.20%\n",
      "Epoch [31/150], Training Loss: 0.6428\n",
      "Epoch [31/150], Validation Loss: 0.9114, Validation Accuracy: 69.56%\n",
      "Epoch [32/150], Training Loss: 0.6296\n",
      "Epoch [32/150], Validation Loss: 0.9056, Validation Accuracy: 70.02%\n",
      "Epoch [33/150], Training Loss: 0.6153\n",
      "Epoch [33/150], Validation Loss: 0.9175, Validation Accuracy: 69.74%\n",
      "Epoch [34/150], Training Loss: 0.6040\n",
      "Epoch [34/150], Validation Loss: 0.9234, Validation Accuracy: 69.80%\n",
      "Epoch [35/150], Training Loss: 0.5926\n",
      "Epoch [35/150], Validation Loss: 0.9089, Validation Accuracy: 69.42%\n",
      "Epoch [36/150], Training Loss: 0.5833\n",
      "Epoch [36/150], Validation Loss: 0.8936, Validation Accuracy: 70.34%\n",
      "Epoch [37/150], Training Loss: 0.5667\n",
      "Epoch [37/150], Validation Loss: 0.9148, Validation Accuracy: 69.70%\n",
      "Epoch [38/150], Training Loss: 0.5603\n",
      "Epoch [38/150], Validation Loss: 0.9252, Validation Accuracy: 69.48%\n",
      "Epoch [39/150], Training Loss: 0.5485\n",
      "Epoch [39/150], Validation Loss: 0.9144, Validation Accuracy: 70.00%\n",
      "Epoch [40/150], Training Loss: 0.5410\n",
      "Epoch [40/150], Validation Loss: 0.9125, Validation Accuracy: 70.52%\n",
      "Epoch [41/150], Training Loss: 0.5261\n",
      "Epoch [41/150], Validation Loss: 0.9058, Validation Accuracy: 71.62%\n",
      "Epoch [42/150], Training Loss: 0.5160\n",
      "Epoch [42/150], Validation Loss: 0.9063, Validation Accuracy: 70.76%\n",
      "Epoch [43/150], Training Loss: 0.5077\n",
      "Epoch [43/150], Validation Loss: 0.8986, Validation Accuracy: 70.64%\n",
      "Epoch [44/150], Training Loss: 0.4996\n",
      "Epoch [44/150], Validation Loss: 0.8938, Validation Accuracy: 70.66%\n",
      "Epoch [45/150], Training Loss: 0.4871\n",
      "Epoch [45/150], Validation Loss: 0.9238, Validation Accuracy: 71.42%\n",
      "Epoch [46/150], Training Loss: 0.4801\n",
      "Epoch [46/150], Validation Loss: 0.9169, Validation Accuracy: 71.20%\n",
      "Epoch [47/150], Training Loss: 0.4713\n",
      "Epoch [47/150], Validation Loss: 0.9525, Validation Accuracy: 70.42%\n",
      "Epoch [48/150], Training Loss: 0.4653\n",
      "Epoch [48/150], Validation Loss: 0.9200, Validation Accuracy: 71.84%\n",
      "Epoch [49/150], Training Loss: 0.4452\n",
      "Epoch [49/150], Validation Loss: 0.9327, Validation Accuracy: 71.26%\n",
      "Epoch [50/150], Training Loss: 0.4470\n",
      "Epoch [50/150], Validation Loss: 0.9436, Validation Accuracy: 70.78%\n",
      "Epoch [51/150], Training Loss: 0.4336\n",
      "Epoch [51/150], Validation Loss: 0.9236, Validation Accuracy: 70.98%\n",
      "Epoch [52/150], Training Loss: 0.4281\n",
      "Epoch [52/150], Validation Loss: 0.9301, Validation Accuracy: 71.10%\n",
      "Epoch [53/150], Training Loss: 0.4190\n",
      "Epoch [53/150], Validation Loss: 0.9385, Validation Accuracy: 72.12%\n",
      "Epoch [54/150], Training Loss: 0.4148\n",
      "Epoch [54/150], Validation Loss: 0.9273, Validation Accuracy: 71.50%\n",
      "Epoch [55/150], Training Loss: 0.4019\n",
      "Epoch [55/150], Validation Loss: 0.9512, Validation Accuracy: 71.18%\n",
      "Epoch [56/150], Training Loss: 0.3956\n",
      "Epoch [56/150], Validation Loss: 0.9820, Validation Accuracy: 70.80%\n",
      "Epoch [57/150], Training Loss: 0.3975\n",
      "Epoch [57/150], Validation Loss: 0.9822, Validation Accuracy: 70.32%\n",
      "Epoch [58/150], Training Loss: 0.3845\n",
      "Epoch [58/150], Validation Loss: 0.9181, Validation Accuracy: 72.16%\n",
      "Epoch [59/150], Training Loss: 0.3780\n",
      "Epoch [59/150], Validation Loss: 0.9841, Validation Accuracy: 71.28%\n",
      "Epoch [60/150], Training Loss: 0.3689\n",
      "Epoch [60/150], Validation Loss: 0.9459, Validation Accuracy: 71.58%\n",
      "Epoch [61/150], Training Loss: 0.3699\n",
      "Epoch [61/150], Validation Loss: 0.9939, Validation Accuracy: 71.10%\n",
      "Epoch [62/150], Training Loss: 0.3594\n",
      "Epoch [62/150], Validation Loss: 0.9653, Validation Accuracy: 71.36%\n",
      "Epoch [63/150], Training Loss: 0.3531\n",
      "Epoch [63/150], Validation Loss: 0.9317, Validation Accuracy: 72.24%\n",
      "Epoch [64/150], Training Loss: 0.3504\n",
      "Epoch [64/150], Validation Loss: 0.9774, Validation Accuracy: 71.78%\n",
      "Epoch [65/150], Training Loss: 0.3368\n",
      "Epoch [65/150], Validation Loss: 0.9948, Validation Accuracy: 71.40%\n",
      "Epoch [66/150], Training Loss: 0.3347\n",
      "Epoch [66/150], Validation Loss: 1.0241, Validation Accuracy: 70.48%\n",
      "Epoch [67/150], Training Loss: 0.3301\n",
      "Epoch [67/150], Validation Loss: 1.0519, Validation Accuracy: 71.28%\n",
      "Epoch [68/150], Training Loss: 0.3236\n",
      "Epoch [68/150], Validation Loss: 1.0081, Validation Accuracy: 71.56%\n",
      "Epoch [69/150], Training Loss: 0.3199\n",
      "Epoch [69/150], Validation Loss: 1.0345, Validation Accuracy: 70.82%\n",
      "Epoch [70/150], Training Loss: 0.3181\n",
      "Epoch [70/150], Validation Loss: 1.0487, Validation Accuracy: 70.40%\n",
      "Epoch [71/150], Training Loss: 0.3109\n",
      "Epoch [71/150], Validation Loss: 1.0138, Validation Accuracy: 71.58%\n",
      "Epoch [72/150], Training Loss: 0.3036\n",
      "Epoch [72/150], Validation Loss: 0.9940, Validation Accuracy: 71.54%\n",
      "Epoch [73/150], Training Loss: 0.3001\n",
      "Epoch [73/150], Validation Loss: 0.9994, Validation Accuracy: 71.92%\n",
      "Epoch [74/150], Training Loss: 0.2936\n",
      "Epoch [74/150], Validation Loss: 1.0004, Validation Accuracy: 72.50%\n",
      "Epoch [75/150], Training Loss: 0.2865\n",
      "Epoch [75/150], Validation Loss: 1.0030, Validation Accuracy: 72.52%\n",
      "Epoch [76/150], Training Loss: 0.2870\n",
      "Epoch [76/150], Validation Loss: 1.0306, Validation Accuracy: 71.62%\n",
      "Epoch [77/150], Training Loss: 0.2810\n",
      "Epoch [77/150], Validation Loss: 1.0080, Validation Accuracy: 71.76%\n",
      "Epoch [78/150], Training Loss: 0.2727\n",
      "Epoch [78/150], Validation Loss: 1.0280, Validation Accuracy: 71.56%\n",
      "Epoch [79/150], Training Loss: 0.2795\n",
      "Epoch [79/150], Validation Loss: 1.0204, Validation Accuracy: 72.30%\n",
      "Epoch [80/150], Training Loss: 0.2698\n",
      "Epoch [80/150], Validation Loss: 1.0563, Validation Accuracy: 71.54%\n",
      "Epoch [81/150], Training Loss: 0.2002\n",
      "Epoch [81/150], Validation Loss: 0.9806, Validation Accuracy: 73.40%\n",
      "Epoch [82/150], Training Loss: 0.1714\n",
      "Epoch [82/150], Validation Loss: 0.9933, Validation Accuracy: 73.38%\n",
      "Epoch [83/150], Training Loss: 0.1620\n",
      "Epoch [83/150], Validation Loss: 1.0270, Validation Accuracy: 73.60%\n",
      "Epoch [84/150], Training Loss: 0.1577\n",
      "Epoch [84/150], Validation Loss: 1.0376, Validation Accuracy: 73.18%\n",
      "Epoch [85/150], Training Loss: 0.1501\n",
      "Epoch [85/150], Validation Loss: 1.0294, Validation Accuracy: 73.86%\n",
      "Epoch [86/150], Training Loss: 0.1451\n",
      "Epoch [86/150], Validation Loss: 1.0292, Validation Accuracy: 73.18%\n",
      "Epoch [87/150], Training Loss: 0.1413\n",
      "Epoch [87/150], Validation Loss: 1.0519, Validation Accuracy: 73.30%\n",
      "Epoch [88/150], Training Loss: 0.1352\n",
      "Epoch [88/150], Validation Loss: 1.0369, Validation Accuracy: 73.48%\n",
      "Epoch [89/150], Training Loss: 0.1329\n",
      "Epoch [89/150], Validation Loss: 1.0769, Validation Accuracy: 73.52%\n",
      "Epoch [90/150], Training Loss: 0.1285\n",
      "Epoch [90/150], Validation Loss: 1.0830, Validation Accuracy: 73.22%\n",
      "Epoch [91/150], Training Loss: 0.1302\n",
      "Epoch [91/150], Validation Loss: 1.0712, Validation Accuracy: 73.74%\n",
      "Epoch [92/150], Training Loss: 0.1246\n",
      "Epoch [92/150], Validation Loss: 1.0762, Validation Accuracy: 73.60%\n",
      "Epoch [93/150], Training Loss: 0.1223\n",
      "Epoch [93/150], Validation Loss: 1.0939, Validation Accuracy: 73.42%\n",
      "Epoch [94/150], Training Loss: 0.1240\n",
      "Epoch [94/150], Validation Loss: 1.1096, Validation Accuracy: 73.38%\n",
      "Epoch [95/150], Training Loss: 0.1139\n",
      "Epoch [95/150], Validation Loss: 1.0929, Validation Accuracy: 74.02%\n",
      "Epoch [96/150], Training Loss: 0.1173\n",
      "Epoch [96/150], Validation Loss: 1.0833, Validation Accuracy: 73.38%\n",
      "Epoch [97/150], Training Loss: 0.1148\n",
      "Epoch [97/150], Validation Loss: 1.1129, Validation Accuracy: 73.24%\n",
      "Epoch [98/150], Training Loss: 0.1140\n",
      "Epoch [98/150], Validation Loss: 1.0968, Validation Accuracy: 73.82%\n",
      "Epoch [99/150], Training Loss: 0.1082\n",
      "Epoch [99/150], Validation Loss: 1.0973, Validation Accuracy: 73.50%\n",
      "Epoch [100/150], Training Loss: 0.1059\n",
      "Epoch [100/150], Validation Loss: 1.1145, Validation Accuracy: 73.34%\n",
      "Epoch [101/150], Training Loss: 0.1098\n",
      "Epoch [101/150], Validation Loss: 1.1246, Validation Accuracy: 73.56%\n",
      "Epoch [102/150], Training Loss: 0.1064\n",
      "Epoch [102/150], Validation Loss: 1.1639, Validation Accuracy: 73.04%\n",
      "Epoch [103/150], Training Loss: 0.1046\n",
      "Epoch [103/150], Validation Loss: 1.1358, Validation Accuracy: 73.40%\n",
      "Epoch [104/150], Training Loss: 0.1038\n",
      "Epoch [104/150], Validation Loss: 1.1430, Validation Accuracy: 73.58%\n",
      "Epoch [105/150], Training Loss: 0.1005\n",
      "Epoch [105/150], Validation Loss: 1.1416, Validation Accuracy: 73.24%\n",
      "Epoch [106/150], Training Loss: 0.0983\n",
      "Epoch [106/150], Validation Loss: 1.1619, Validation Accuracy: 73.68%\n",
      "Epoch [107/150], Training Loss: 0.0998\n",
      "Epoch [107/150], Validation Loss: 1.1506, Validation Accuracy: 72.86%\n",
      "Epoch [108/150], Training Loss: 0.0961\n",
      "Epoch [108/150], Validation Loss: 1.1693, Validation Accuracy: 73.08%\n",
      "Epoch [109/150], Training Loss: 0.0960\n",
      "Epoch [109/150], Validation Loss: 1.1525, Validation Accuracy: 73.42%\n",
      "Epoch [110/150], Training Loss: 0.0952\n",
      "Epoch [110/150], Validation Loss: 1.1363, Validation Accuracy: 73.34%\n",
      "Epoch [111/150], Training Loss: 0.0938\n",
      "Epoch [111/150], Validation Loss: 1.1478, Validation Accuracy: 73.66%\n",
      "Epoch [112/150], Training Loss: 0.0937\n",
      "Epoch [112/150], Validation Loss: 1.1786, Validation Accuracy: 72.62%\n",
      "Epoch [113/150], Training Loss: 0.0960\n",
      "Epoch [113/150], Validation Loss: 1.1389, Validation Accuracy: 74.38%\n",
      "Epoch [114/150], Training Loss: 0.0963\n",
      "Epoch [114/150], Validation Loss: 1.1423, Validation Accuracy: 73.98%\n",
      "Epoch [115/150], Training Loss: 0.0920\n",
      "Epoch [115/150], Validation Loss: 1.2146, Validation Accuracy: 73.08%\n",
      "Epoch [116/150], Training Loss: 0.0907\n",
      "Epoch [116/150], Validation Loss: 1.1713, Validation Accuracy: 74.14%\n",
      "Epoch [117/150], Training Loss: 0.0881\n",
      "Epoch [117/150], Validation Loss: 1.1700, Validation Accuracy: 74.02%\n",
      "Epoch [118/150], Training Loss: 0.0885\n",
      "Epoch [118/150], Validation Loss: 1.2050, Validation Accuracy: 73.46%\n",
      "Epoch [119/150], Training Loss: 0.0873\n",
      "Epoch [119/150], Validation Loss: 1.1751, Validation Accuracy: 73.90%\n",
      "Epoch [120/150], Training Loss: 0.0871\n",
      "Epoch [120/150], Validation Loss: 1.2048, Validation Accuracy: 73.16%\n",
      "Epoch [121/150], Training Loss: 0.0857\n",
      "Epoch [121/150], Validation Loss: 1.1955, Validation Accuracy: 73.90%\n",
      "Epoch [122/150], Training Loss: 0.0888\n",
      "Epoch [122/150], Validation Loss: 1.1778, Validation Accuracy: 74.22%\n",
      "Epoch [123/150], Training Loss: 0.0855\n",
      "Epoch [123/150], Validation Loss: 1.2173, Validation Accuracy: 73.90%\n",
      "Epoch [124/150], Training Loss: 0.0855\n",
      "Epoch [124/150], Validation Loss: 1.2119, Validation Accuracy: 73.58%\n",
      "Epoch [125/150], Training Loss: 0.0850\n",
      "Epoch [125/150], Validation Loss: 1.2192, Validation Accuracy: 73.44%\n",
      "Epoch [126/150], Training Loss: 0.0843\n",
      "Epoch [126/150], Validation Loss: 1.2356, Validation Accuracy: 73.58%\n",
      "Epoch [127/150], Training Loss: 0.0839\n",
      "Epoch [127/150], Validation Loss: 1.2065, Validation Accuracy: 73.96%\n",
      "Epoch [128/150], Training Loss: 0.0804\n",
      "Epoch [128/150], Validation Loss: 1.1999, Validation Accuracy: 73.98%\n",
      "Epoch [129/150], Training Loss: 0.0794\n",
      "Epoch [129/150], Validation Loss: 1.2208, Validation Accuracy: 74.02%\n",
      "Epoch [130/150], Training Loss: 0.0791\n",
      "Epoch [130/150], Validation Loss: 1.2553, Validation Accuracy: 73.84%\n",
      "Epoch [131/150], Training Loss: 0.0794\n",
      "Epoch [131/150], Validation Loss: 1.2067, Validation Accuracy: 73.92%\n",
      "Epoch [132/150], Training Loss: 0.0776\n",
      "Epoch [132/150], Validation Loss: 1.1839, Validation Accuracy: 74.04%\n",
      "Epoch [133/150], Training Loss: 0.0764\n",
      "Epoch [133/150], Validation Loss: 1.2115, Validation Accuracy: 73.54%\n",
      "Epoch [134/150], Training Loss: 0.0748\n",
      "Epoch [134/150], Validation Loss: 1.1721, Validation Accuracy: 73.82%\n",
      "Epoch [135/150], Training Loss: 0.0733\n",
      "Epoch [135/150], Validation Loss: 1.1770, Validation Accuracy: 74.46%\n",
      "Epoch [136/150], Training Loss: 0.0740\n",
      "Epoch [136/150], Validation Loss: 1.2385, Validation Accuracy: 73.68%\n",
      "Epoch [137/150], Training Loss: 0.0741\n",
      "Epoch [137/150], Validation Loss: 1.1922, Validation Accuracy: 73.50%\n",
      "Epoch [138/150], Training Loss: 0.0743\n",
      "Epoch [138/150], Validation Loss: 1.2139, Validation Accuracy: 74.32%\n",
      "Epoch [139/150], Training Loss: 0.0744\n",
      "Epoch [139/150], Validation Loss: 1.2394, Validation Accuracy: 73.30%\n",
      "Epoch [140/150], Training Loss: 0.0733\n",
      "Epoch [140/150], Validation Loss: 1.2017, Validation Accuracy: 74.46%\n",
      "Epoch [141/150], Training Loss: 0.0748\n",
      "Epoch [141/150], Validation Loss: 1.2100, Validation Accuracy: 74.30%\n",
      "Epoch [142/150], Training Loss: 0.0745\n",
      "Epoch [142/150], Validation Loss: 1.2148, Validation Accuracy: 73.82%\n",
      "Epoch [143/150], Training Loss: 0.0702\n",
      "Epoch [143/150], Validation Loss: 1.1930, Validation Accuracy: 73.54%\n",
      "Epoch [144/150], Training Loss: 0.0741\n",
      "Epoch [144/150], Validation Loss: 1.2268, Validation Accuracy: 74.28%\n",
      "Epoch [145/150], Training Loss: 0.0711\n",
      "Epoch [145/150], Validation Loss: 1.2005, Validation Accuracy: 74.30%\n",
      "Epoch [146/150], Training Loss: 0.0721\n",
      "Epoch [146/150], Validation Loss: 1.2021, Validation Accuracy: 73.56%\n",
      "Epoch [147/150], Training Loss: 0.0692\n",
      "Epoch [147/150], Validation Loss: 1.2096, Validation Accuracy: 73.98%\n",
      "Epoch [148/150], Training Loss: 0.0691\n",
      "Epoch [148/150], Validation Loss: 1.2201, Validation Accuracy: 73.78%\n",
      "Epoch [149/150], Training Loss: 0.0696\n",
      "Epoch [149/150], Validation Loss: 1.2306, Validation Accuracy: 73.82%\n",
      "Epoch [150/150], Training Loss: 0.0704\n",
      "Epoch [150/150], Validation Loss: 1.2389, Validation Accuracy: 74.00%\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 150  # example value, adjust as needed\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    # Training Phase\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    for i, (inputs, labels) in enumerate(train_loader):\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward pass and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    avg_train_loss = total_loss / len(train_loader)\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Training Loss: {avg_train_loss:.4f}\")\n",
    "\n",
    "    # Validation Phase\n",
    "    model.eval()\n",
    "    total_val_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            total_val_loss += loss.item()\n",
    "\n",
    "            _, predicted = outputs.max(dim=-1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "    avg_val_loss = total_val_loss / len(val_loader)\n",
    "    val_accuracy = 100 * correct / total\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Validation Loss: {avg_val_loss:.4f}, Validation Accuracy: {val_accuracy:.2f}%\")\n",
    "\n",
    "    # Update the learning rate\n",
    "    lr_scheduler.step()\n",
    "\n",
    "print(\"Training complete!\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-01T14:20:31.418683289Z",
     "start_time": "2023-11-01T13:55:42.579259873Z"
    }
   },
   "id": "b675ed45e4aa2ed3"
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 75.07%\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:  \n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = outputs.max(dim=-1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "test_accuracy = 100 * correct / total\n",
    "print(f\"Test Accuracy: {test_accuracy:.2f}%\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-01T14:20:37.270654939Z",
     "start_time": "2023-11-01T14:20:35.926145931Z"
    }
   },
   "id": "c3edea32ec9d3337"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "8fb3268d389ffe72"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
