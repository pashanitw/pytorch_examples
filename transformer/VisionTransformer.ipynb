{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-10-30T16:33:40.052168693Z",
     "start_time": "2023-10-30T16:33:40.005268246Z"
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import torch \n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import CIFAR10\n",
    "from torch import nn\n",
    "from dataclasses import dataclass\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "outputs": [],
   "source": [
    "def img_to_patch(x, patch_size, flatten_channels=True):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "        x - torch.Tensor representing the image of shape [B, C, H, W]\n",
    "        patch_size - Number of pixels per dimension of the patches (integer)\n",
    "        flatten_channels - If True, the patches will be returned in a flattened format\n",
    "                           as a feature vector instead of a image grid.\n",
    "    \"\"\"\n",
    "    B, C, H, W = x.shape\n",
    "    x = x.reshape(B, C, H//patch_size, patch_size, W//patch_size, patch_size)\n",
    "    x = x.permute(0, 2, 4, 1, 3, 5) # [B, H', W', C, p_H, p_W]\n",
    "    x = x.flatten(1,2)              # [B, H'*W', C, p_H, p_W]\n",
    "    if flatten_channels:\n",
    "        x = x.flatten(2,4)          # [B, H'*W', C*p_H*p_W]\n",
    "    return x"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-30T16:33:41.643288594Z",
     "start_time": "2023-10-30T16:33:41.640484224Z"
    }
   },
   "id": "59db38cddc698b1e"
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "outputs": [
    {
     "data": {
      "text/plain": "[0, 1, 2, 3, 4, 5, 6, 7]"
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_classes = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "desired_classes = all_classes[0:8]\n",
    "desired_indices = [all_classes.index(cls) for cls in desired_classes]\n",
    "desired_indices"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-30T16:33:51.929353338Z",
     "start_time": "2023-10-30T16:33:51.925594921Z"
    }
   },
   "id": "44ee1ebfd4ac50f9"
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "outputs": [],
   "source": [
    "DATA_DIR=\"../data\"\n",
    "def get_cifar10_data_loader():\n",
    "    \"\"\"\n",
    "    Get the CIFAR10 data loader\n",
    "    \"\"\"\n",
    "    # define the transform\n",
    "    transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "\n",
    "    # get the training and testing datasets\n",
    "    train_dataset = CIFAR10(root=DATA_DIR, train=True, transform=transform, download=True)\n",
    "    test_dataset = CIFAR10(root=DATA_DIR, train=False, transform=transform, download=True)\n",
    "    train_set, val_set = torch.utils.data.random_split(train_dataset, [45000, 5000])\n",
    "\n",
    "    return train_set, val_set, test_dataset"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-30T16:33:53.441729860Z",
     "start_time": "2023-10-30T16:33:53.437915692Z"
    }
   },
   "id": "699af8d3eeb56e96"
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "train_set, val_set, test_set = get_cifar10_data_loader()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-30T16:10:42.177750340Z",
     "start_time": "2023-10-30T16:10:41.178927433Z"
    }
   },
   "id": "7bd8ba9538787de"
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "outputs": [],
   "source": [
    "def filter_dataset(dataset):\n",
    "    filtered_indices = [i  for i,  (_, label) in enumerate(dataset) if label in desired_indices]\n",
    "    return torch.utils.data.Subset(dataset, filtered_indices)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-30T16:10:45.342350866Z",
     "start_time": "2023-10-30T16:10:45.337273252Z"
    }
   },
   "id": "d607719ad031923"
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len of train set 36029 val set 3971 test set 8000\n"
     ]
    }
   ],
   "source": [
    "train_set = filter_dataset(train_set)\n",
    "val_set = filter_dataset(val_set)\n",
    "test_set = filter_dataset(test_set)\n",
    "print(f\"len of train set {len(train_set)} val set {len(val_set)} test set {len(test_set)}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-30T16:10:49.034806909Z",
     "start_time": "2023-10-30T16:10:45.738142750Z"
    }
   },
   "id": "f2a78ac8c95292f8"
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ModelArgs:\n",
    "    dim:int =  256\n",
    "    hidden_dim:int = 512\n",
    "    n_heads:int = 8\n",
    "    n_layers:int = 6\n",
    "    patch_size:int = 4\n",
    "    n_channels = 3\n",
    "    n_patches = 64\n",
    "    n_classes = 10\n",
    "    dropout = 0.2"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-30T16:34:05.029604766Z",
     "start_time": "2023-10-30T16:34:05.027162641Z"
    }
   },
   "id": "c973a30501c1fd03"
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, args:ModelArgs):\n",
    "        super().__init__()\n",
    "        self.n_heads = args.n_heads\n",
    "        self.dim = args.dim\n",
    "        self.head_dim = args.dim // args.n_heads\n",
    "        \n",
    "        self.wq = nn.Linear(self.dim, self.n_heads*self.head_dim, bias=False)\n",
    "        self.wk = nn.Linear(self.dim, self.n_heads*self.head_dim, bias=False)\n",
    "        self.wv = nn.Linear(self.dim, self.n_heads*self.head_dim, bias=False)\n",
    "        self.wo = nn.Linear(self.n_heads*self.head_dim, self.dim, bias=False)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        b, seq_len, dim = x.shape\n",
    "        \n",
    "        assert dim == self.dim, \"dim is not matching\"\n",
    "        q = self.wq(x)\n",
    "        k = self.wk(x)\n",
    "        v = self.wv(x)\n",
    "        \n",
    "        q = q.contiguous().view(b, seq_len, self.n_heads, self.head_dim)\n",
    "        k = k.contiguous().view(b, seq_len, self.n_heads, self.head_dim)\n",
    "        v = v.contiguous().view(b, seq_len, self.n_heads, self.head_dim)\n",
    "        \n",
    "        q = q.transpose(1, 2)\n",
    "        k = k.transpose(1, 2)\n",
    "        v = v.transpose(1,2)\n",
    "        \n",
    "        attn = torch.matmul(q, k. transpose(2, 3)) / math.sqrt(self.head_dim)\n",
    "        attn_scores = F.softmax(attn, dim = -1)\n",
    "        \n",
    "        out = torch.matmul(attn_scores, v)\n",
    "        out = out.contiguous().view(b, seq_len, -1)\n",
    "        \n",
    "        return self.wo(out)        "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-30T16:34:05.529235760Z",
     "start_time": "2023-10-30T16:34:05.527256935Z"
    }
   },
   "id": "19ec9440213a820a"
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "outputs": [],
   "source": [
    "class AttentionBlock(nn.Module):\n",
    "    def __init__(self, args: ModelArgs):\n",
    "        super().__init__()\n",
    "        self.layer_norm_1 = nn.LayerNorm(args.dim)\n",
    "        self.attn = MultiHeadAttention(args)\n",
    "        \n",
    "        self.layer_norm_2 = nn.LayerNorm(args.dim)\n",
    "        \n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(args.dim, args.hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(args.dropout),\n",
    "            nn.Linear(args.hidden_dim, args.dim),\n",
    "            nn.Dropout(args.dropout)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.layer_norm_1(x))\n",
    "        x = x + self.ffn(self.layer_norm_2(x))\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-30T16:34:05.885021374Z",
     "start_time": "2023-10-30T16:34:05.879633508Z"
    }
   },
   "id": "4c3f7596b4ec9009"
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "outputs": [],
   "source": [
    "class VisionTransformer(nn.Module):\n",
    "    def __init__(self, args: ModelArgs):\n",
    "        super().__init__()\n",
    "        self.patch_size = args.patch_size\n",
    "        \n",
    "        self.input_layer = nn.Linear(args.n_channels * (args.patch_size ** 2), args.dim)\n",
    "        attn_blocks = []\n",
    "        for _ in range(args.n_layers):\n",
    "            attn_blocks.append(AttentionBlock(args))\n",
    "        \n",
    "        self.transformer = nn.Sequential(*attn_blocks)\n",
    "        \n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.LayerNorm(args.dim),\n",
    "            nn.Linear(args.dim, args.n_classes)\n",
    "        )\n",
    "        \n",
    "        self.dropout = nn.Dropout(args.dropout)\n",
    "        \n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, args.dim))\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1, 1+args.n_patches, args.dim))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = img_to_patch(x, self.patch_size)\n",
    "        b, seq_len, _ = x.shape\n",
    "        x = self.input_layer(x)\n",
    "        \n",
    "        cls_token = self.cls_token.repeat(b, 1, 1)\n",
    "        x = torch.cat([cls_token, x], dim=1)\n",
    "        \n",
    "        x = x + self.pos_embedding[:,:seq_len+1]\n",
    "        \n",
    "        x = self.dropout(x)\n",
    "        x = self.transformer(x)\n",
    "        # print(\"========== x shape =====\", x.shape)\n",
    "        x = x.transpose(0, 1)\n",
    "        cls = x[0]\n",
    "        out = self.mlp(cls)\n",
    "        return out"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-30T16:34:06.135779615Z",
     "start_time": "2023-10-30T16:34:06.123705258Z"
    }
   },
   "id": "b51bde2d3d36e9c9"
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "outputs": [
    {
     "data": {
      "text/plain": "256"
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args = ModelArgs()\n",
    "args.dim"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-30T16:34:06.765051993Z",
     "start_time": "2023-10-30T16:34:06.760292748Z"
    }
   },
   "id": "228d8e565b8ac835"
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "outputs": [],
   "source": [
    "# Model, Loss and Optimizer\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else 0\n",
    "args = ModelArgs()\n",
    "model = VisionTransformer(args).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=3e-4)\n",
    "lr_scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[100, 150], gamma=0.1)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-30T16:34:07.189709079Z",
     "start_time": "2023-10-30T16:34:07.175316910Z"
    }
   },
   "id": "816aefa5793746e0"
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "outputs": [],
   "source": [
    "batch_size=64\n",
    "num_workers = 16\n",
    "# get the data loaders\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_set, batch_size=batch_size, shuffle=True,\n",
    "                                           num_workers=num_workers, drop_last=True)\n",
    "val_loader = torch.utils.data.DataLoader(dataset=val_set, batch_size=batch_size, shuffle=True,\n",
    "                                         num_workers=num_workers, drop_last=False)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_set, batch_size=batch_size, shuffle=False,\n",
    "                                          num_workers=num_workers, drop_last=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-30T16:34:07.643938830Z",
     "start_time": "2023-10-30T16:34:07.628946200Z"
    }
   },
   "id": "1388884734ae19ea"
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/200], Training Loss: 1.6599\n",
      "Epoch [1/200], Validation Loss: 1.5112, Validation Accuracy: 43.21%\n",
      "Epoch [2/200], Training Loss: 1.3780\n",
      "Epoch [2/200], Validation Loss: 1.2998, Validation Accuracy: 51.67%\n",
      "Epoch [3/200], Training Loss: 1.2626\n",
      "Epoch [3/200], Validation Loss: 1.2616, Validation Accuracy: 55.20%\n",
      "Epoch [4/200], Training Loss: 1.1883\n",
      "Epoch [4/200], Validation Loss: 1.2068, Validation Accuracy: 56.06%\n",
      "Epoch [5/200], Training Loss: 1.1228\n",
      "Epoch [5/200], Validation Loss: 1.2128, Validation Accuracy: 58.47%\n",
      "Epoch [6/200], Training Loss: 1.0702\n",
      "Epoch [6/200], Validation Loss: 1.1508, Validation Accuracy: 58.93%\n",
      "Epoch [7/200], Training Loss: 1.0143\n",
      "Epoch [7/200], Validation Loss: 1.1241, Validation Accuracy: 59.28%\n",
      "Epoch [8/200], Training Loss: 0.9619\n",
      "Epoch [8/200], Validation Loss: 1.1036, Validation Accuracy: 59.81%\n",
      "Epoch [9/200], Training Loss: 0.9168\n",
      "Epoch [9/200], Validation Loss: 1.1206, Validation Accuracy: 60.09%\n",
      "Epoch [10/200], Training Loss: 0.8715\n",
      "Epoch [10/200], Validation Loss: 1.1410, Validation Accuracy: 60.79%\n",
      "Epoch [11/200], Training Loss: 0.8223\n",
      "Epoch [11/200], Validation Loss: 1.1474, Validation Accuracy: 60.97%\n",
      "Epoch [12/200], Training Loss: 0.7823\n",
      "Epoch [12/200], Validation Loss: 1.1457, Validation Accuracy: 60.77%\n",
      "Epoch [13/200], Training Loss: 0.7370\n",
      "Epoch [13/200], Validation Loss: 1.1471, Validation Accuracy: 60.99%\n",
      "Epoch [14/200], Training Loss: 0.6933\n",
      "Epoch [14/200], Validation Loss: 1.1763, Validation Accuracy: 60.61%\n",
      "Epoch [15/200], Training Loss: 0.6482\n",
      "Epoch [15/200], Validation Loss: 1.2206, Validation Accuracy: 60.29%\n",
      "Epoch [16/200], Training Loss: 0.6040\n",
      "Epoch [16/200], Validation Loss: 1.2401, Validation Accuracy: 59.46%\n",
      "Epoch [17/200], Training Loss: 0.5648\n",
      "Epoch [17/200], Validation Loss: 1.2406, Validation Accuracy: 61.22%\n",
      "Epoch [18/200], Training Loss: 0.5246\n",
      "Epoch [18/200], Validation Loss: 1.2988, Validation Accuracy: 60.54%\n",
      "Epoch [19/200], Training Loss: 0.4896\n",
      "Epoch [19/200], Validation Loss: 1.3623, Validation Accuracy: 58.93%\n",
      "Epoch [20/200], Training Loss: 0.4561\n",
      "Epoch [20/200], Validation Loss: 1.4314, Validation Accuracy: 59.23%\n",
      "Epoch [21/200], Training Loss: 0.4319\n",
      "Epoch [21/200], Validation Loss: 1.3958, Validation Accuracy: 61.22%\n",
      "Epoch [22/200], Training Loss: 0.3933\n",
      "Epoch [22/200], Validation Loss: 1.4577, Validation Accuracy: 59.73%\n",
      "Epoch [23/200], Training Loss: 0.3731\n",
      "Epoch [23/200], Validation Loss: 1.4830, Validation Accuracy: 59.66%\n",
      "Epoch [24/200], Training Loss: 0.3402\n",
      "Epoch [24/200], Validation Loss: 1.5218, Validation Accuracy: 59.51%\n",
      "Epoch [25/200], Training Loss: 0.3251\n",
      "Epoch [25/200], Validation Loss: 1.5452, Validation Accuracy: 59.78%\n",
      "Epoch [26/200], Training Loss: 0.2965\n",
      "Epoch [26/200], Validation Loss: 1.7093, Validation Accuracy: 59.51%\n",
      "Epoch [27/200], Training Loss: 0.2813\n",
      "Epoch [27/200], Validation Loss: 1.6228, Validation Accuracy: 59.91%\n",
      "Epoch [28/200], Training Loss: 0.2682\n",
      "Epoch [28/200], Validation Loss: 1.6925, Validation Accuracy: 59.93%\n",
      "Epoch [29/200], Training Loss: 0.2519\n",
      "Epoch [29/200], Validation Loss: 1.6607, Validation Accuracy: 60.19%\n",
      "Epoch [30/200], Training Loss: 0.2427\n",
      "Epoch [30/200], Validation Loss: 1.7807, Validation Accuracy: 59.28%\n",
      "Epoch [31/200], Training Loss: 0.2230\n",
      "Epoch [31/200], Validation Loss: 1.8046, Validation Accuracy: 60.24%\n",
      "Epoch [32/200], Training Loss: 0.2175\n",
      "Epoch [32/200], Validation Loss: 1.8329, Validation Accuracy: 59.38%\n",
      "Epoch [33/200], Training Loss: 0.2084\n",
      "Epoch [33/200], Validation Loss: 1.8190, Validation Accuracy: 60.74%\n",
      "Epoch [34/200], Training Loss: 0.2003\n",
      "Epoch [34/200], Validation Loss: 1.8205, Validation Accuracy: 59.53%\n",
      "Epoch [35/200], Training Loss: 0.1990\n",
      "Epoch [35/200], Validation Loss: 1.9307, Validation Accuracy: 58.63%\n",
      "Epoch [36/200], Training Loss: 0.1848\n",
      "Epoch [36/200], Validation Loss: 1.9217, Validation Accuracy: 60.39%\n",
      "Epoch [37/200], Training Loss: 0.1815\n",
      "Epoch [37/200], Validation Loss: 1.8460, Validation Accuracy: 59.81%\n",
      "Epoch [38/200], Training Loss: 0.1728\n",
      "Epoch [38/200], Validation Loss: 1.8697, Validation Accuracy: 58.98%\n",
      "Epoch [39/200], Training Loss: 0.1649\n",
      "Epoch [39/200], Validation Loss: 1.8871, Validation Accuracy: 59.46%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[123], line 23\u001B[0m\n\u001B[1;32m     20\u001B[0m     loss\u001B[38;5;241m.\u001B[39mbackward()\n\u001B[1;32m     21\u001B[0m     optimizer\u001B[38;5;241m.\u001B[39mstep()\n\u001B[0;32m---> 23\u001B[0m     total_loss \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[43mloss\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mitem\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     25\u001B[0m avg_train_loss \u001B[38;5;241m=\u001B[39m total_loss \u001B[38;5;241m/\u001B[39m \u001B[38;5;28mlen\u001B[39m(train_loader)\n\u001B[1;32m     26\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mEpoch [\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mepoch\u001B[38;5;241m+\u001B[39m\u001B[38;5;241m1\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m/\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mnum_epochs\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m], Training Loss: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mavg_train_loss\u001B[38;5;132;01m:\u001B[39;00m\u001B[38;5;124m.4f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "num_epochs = 200  # example value, adjust as needed\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    # Training Phase\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    for i, (inputs, labels) in enumerate(train_loader):\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        # print(\"==== outputs shape ===\", outputs.shape)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward pass and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    avg_train_loss = total_loss / len(train_loader)\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Training Loss: {avg_train_loss:.4f}\")\n",
    "\n",
    "    # Validation Phase\n",
    "    model.eval()\n",
    "    total_val_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:  # Assuming val_loader is defined elsewhere\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            total_val_loss += loss.item()\n",
    "\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "    avg_val_loss = total_val_loss / len(val_loader)\n",
    "    val_accuracy = 100 * correct / total\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Validation Loss: {avg_val_loss:.4f}, Validation Accuracy: {val_accuracy:.2f}%\")\n",
    "\n",
    "    # Update the learning rate\n",
    "    lr_scheduler.step()\n",
    "\n",
    "print(\"Training complete!\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-30T16:39:38.209524138Z",
     "start_time": "2023-10-30T16:34:08.716023788Z"
    }
   },
   "id": "b675ed45e4aa2ed3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-30T16:03:05.768477318Z",
     "start_time": "2023-10-30T16:03:05.765878890Z"
    }
   },
   "id": "c3edea32ec9d3337"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "b654f33dd9d027c5"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
